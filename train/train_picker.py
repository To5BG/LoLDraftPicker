import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import os

from models import DraftPicker
from train.trainers import PickerTrainer
from utils import (
    DraftDataset,
    load_draft_history,
    load_champion_embeddings,
    load_champion_stats,
)
from config import *


def train_picker_model():
    print("=" * 60)
    print("Training Draft Picker Model")
    print("=" * 60)

    # Load champion embeddings generated by the embedding model
    print(f"\nLoading champion embeddings from {EMBEDDINGS_PATH}...")
    champion_embeddings = load_champion_embeddings(EMBEDDINGS_PATH)
    print(f"Loaded embeddings for {len(champion_embeddings)} champions")
    # Load champion stats for CC features
    print(f"\nLoading champion stats from {CHAMPION_STATS_FILE}...")
    champion_stats = load_champion_stats(CHAMPION_STATS_FILE)
    # Load draft history
    print(f"\nLoading draft history from {DRAFT_HISTORY_FILE}...")
    draft_data = load_draft_history(DRAFT_HISTORY_FILE)
    print(f"Loaded {len(draft_data)} draft scenarios")
    # Create dataset with specific role structure
    dataset = DraftDataset(draft_data, champion_embeddings, champion_stats)
    # Split into train/val
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    # Create dataloaders (no custom collate_fn needed - fixed-size inputs)
    train_loader = DataLoader(
        train_dataset,
        batch_size=PICKER_CONFIG["batch_size"],
        shuffle=True,
    )
    val_loader = DataLoader(val_dataset, batch_size=PICKER_CONFIG["batch_size"])
    # Initialize model
    model = DraftPicker(
        embedding_dim=EMBEDDING_CONFIG["embedding_dim"],
        hidden_dims=PICKER_CONFIG["hidden_dims"],
        output_dim=EMBEDDING_CONFIG["embedding_dim"],
        dropout=PICKER_CONFIG["dropout"],
    )
    # Setup training
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nUsing device: {device}")
    trainer = PickerTrainer(model, device)
    optimizer = optim.Adam(model.parameters(), lr=PICKER_CONFIG["learning_rate"])
    criterion = nn.MSELoss()
    # Training loop
    best_val_loss = float("inf")
    print(f"\nTraining for {PICKER_CONFIG['epochs']} epochs...")
    for epoch in range(PICKER_CONFIG["epochs"]):
        train_loss = trainer.train_epoch(train_loader, optimizer, criterion)
        val_loss = trainer.evaluate(val_loader, criterion)
        print(
            f"Epoch {epoch+1}/{PICKER_CONFIG['epochs']} - "
            f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}"
        )
        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            os.makedirs(MODEL_DIR, exist_ok=True)
            torch.save(model.state_dict(), PICKER_MODEL_PATH)
            print(f"  â†’ Saved new best model (val_loss: {val_loss:.4f})")
    print("\nPicker training complete!")
    print(f"Best validation loss: {best_val_loss:.4f}")


if __name__ == "__main__":
    train_picker_model()
